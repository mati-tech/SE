{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f44a23",
   "metadata": {},
   "source": [
    "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª ‚Äì –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n",
    "–û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞\n",
    "–ü–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—É—é —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. \n",
    "–û–¥–∏–Ω–æ—á–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —è–≤–ª—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–º –±–∏–Ω–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º. –í \n",
    "—ç—Ç–æ–π –ª–µ–∫—Ü–∏–∏ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–æ—Ü–µ–¥—É—Ä—É –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –¥–ª—è \n",
    "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ü–æ—Å–∫–æ–ª—å–∫—É –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π \n",
    "–±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, —Ç–æ –º—ã –±—É–¥–µ–º —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –ª–∏—à—å –¥–≤–∞ –∫–ª–∞—Å—Å–∞.\n",
    "–ü—É—Å—Ç—å –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ (–∫–æ–Ω–µ—á–Ω–æ–µ –∏–ª–∏ \n",
    "–±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ) n-–º–µ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥–µ–º –æ–±–æ–∑–Ω–∞—á–∞—Ç—å ùë• =\n",
    "(ùë•1, ùë•2, . . . , ùë•ùëõ)\n",
    "–ë—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ —ç—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –¥–≤–∞ –∫–ª–∞—Å—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ \n",
    "–º—ã –±—É–¥–µ–º –æ–±–æ–∑–Ω–∞—á–∞—Ç—å +1 –∏ -1. –ü–æ—ç—Ç–æ–º—É –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∑–∞–¥–∞—á–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è \n",
    "—Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–¥–∞–Ω–∞ –Ω–∞ –Ω–∞—à–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤–µ–∫—Ç–æ—Ä–æ–≤, –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç \n",
    "–∑–Ω–∞—á–µ–Ω–∏—è –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–µ {+1, ‚àí1}. –í –∫–∞—á–µ—Å—Ç–≤–µ —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–∂–µ—Ç –≤—ã—Å—Ç—É–ø–∞—Ç—å \n",
    "–ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω. –° –∞–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ \n",
    "–≤–µ—Å–æ–≤ ùë§ = (ùë§0, ùë§1, ùë§2, . . . , ùë§ùëõ).\n",
    "–ü—Ä–∏ —ç—Ç–æ–º –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ —Ñ–æ—Ä–º—É–ª–µ\n",
    "ùë¶ = ùë†ùëñùëîùëõ(ùë§0 + ùë•1ùë§1 + ùë•2ùë§2 + . . . + ùë•ùëõùë§ùëõ),\n",
    "–≥–¥–µ —Ñ—É–Ω–∫—Ü–∏—è ùë†ùëñùëîùëõ(ùë°) —Ä–∞–≤–Ω–∞ +1, –µ—Å–ª–∏ ùë° ‚â• 0, –∏ —Ä–∞–≤–Ω–∞ ‚àí1, –µ—Å–ª–∏ ùë° < 0.\n",
    "–ü—Ä–∏–≤–µ–¥–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞. –ü—É—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä \n",
    "–æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö {(ùë•, ùëë)}, –≥–¥–µ ùë• - —ç—Ç–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞, –∞ ùëë –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ \n",
    "{+1, ‚àí1} —É–∫–∞–∑—ã–≤–∞–µ—Ç –∫ –∫–∞–∫–æ–º—É –∫–ª–∞—Å—Å—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∞—à –≤–µ–∫—Ç–æ—Ä.\n",
    "1. –ü–æ–ª–æ–∂–∏–º –≤–µ–∫—Ç–æ—Ä –≤–µ—Å–æ–≤ ùë§ —Ä–∞–≤–Ω—ã–º –Ω—É–ª—é.\n",
    "2. –ü–æ–≤—Ç–æ—Ä—è—Ç—å ùëÅ —Ä–∞–∑ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (ùë•, ùëë):\n",
    "4. –í—ã—á–∏—Å–ª–∏—Ç—å ùë¶ = ùë†ùëñùëîùëõ[(ùë•, ùë§)].\n",
    "5. –ï—Å–ª–∏ ùë¶ùëë < 0, —Ç–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ ùë§0 = ùë§0 + ùëéùëë, ùë§ùëñ =\n",
    "ùë§ùëñ + ùëéùëëùë•ùëñ\n",
    ", ùëñ = 1,2, . . . , ùëõ. \n",
    "–û–ø–∏—Å–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–æ–≤–æ–ª—å–Ω–æ –ª–µ–≥–∫–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbe8f6",
   "metadata": {},
   "source": [
    "example: \n",
    "1.1.1\n",
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–æ–≥—Ä–∞–º–º—É –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –Ω–∞ —è–∑—ã–∫–µ Python. –°–Ω–∞—á–∞–ª–∞ \n",
    "—Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç —É—á–∏—Ç—å—Å—è –ø–æ \n",
    "—Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3642955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, -0.1]\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification\n",
    "\n",
    "# Class that implements the perceptron and its training\n",
    "class Perceptron:\n",
    "    def __init__(self, N):\n",
    "        # Initialize weights to zero\n",
    "        self.w = [0] * N\n",
    "\n",
    "    # Method to calculate the value of the perceptron\n",
    "    def calc(self, x):\n",
    "        res = 0\n",
    "        for i in range(len(self.w)):\n",
    "            res += self.w[i] * x[i]\n",
    "        return res\n",
    "    \n",
    "    # Threshold activation function of the perceptron (Checking the sign)\n",
    "    def sign(self, x):\n",
    "        if self.calc(x) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    # Training on a single example\n",
    "    def learn(self, learning_rate, x, y):\n",
    "        # Train only when the prediction is incorrect\n",
    "        if y * self.calc(x) <= 0:\n",
    "            for i in range(len(self.w)):\n",
    "                self.w[i] += learning_rate * y * x[i]\n",
    "\n",
    "    # Training on all the data T - list of examples\n",
    "    def learning(self, learning_rate, T):\n",
    "        # Training loop\n",
    "        for _ in range(100):\n",
    "            # Training on the entire set of examples\n",
    "            for t in T:\n",
    "                self.learn(learning_rate, t[0], t[1])\n",
    "\n",
    "# Create a two-dimensional perceptron class\n",
    "\n",
    "perceptron = Perceptron(2)\n",
    "learning_rate = 0.1\n",
    "# Create examples\n",
    "\n",
    "T = []\n",
    "T.append([[2, 1], 1])\n",
    "T.append([[3, 2], 1])\n",
    "T.append([[4, 1], 1])\n",
    "T.append([[1, 2], -1])\n",
    "T.append([[2, 3], -1])\n",
    "T.append([[5, 7], -1])\n",
    "perceptron.learning(learning_rate, T) # Training the perceptron\n",
    "print(perceptron.w) # Print the weights\n",
    "# Test the perceptron on some test examples\n",
    "print(perceptron.sign([1.5, 2]))\n",
    "print(perceptron.sign([3, 1.5]))\n",
    "print(perceptron.sign([5, 1]))\n",
    "print(perceptron.sign([5, 10]))\n",
    "\n",
    "# In this code, we have implemented a simple perceptron for binary classification. The perceptron is trained on a set of two-dimensional examples (T), where each example consists of an input vector and a target class label.\n",
    "\n",
    "# The Perceptron class has methods to calculate the value of the perceptron (calc), apply the threshold activation function (sign), perform training on a single example (learn), and train on the entire dataset (learning).\n",
    "\n",
    "# In the learning method, the perceptron is trained by iterating over the dataset multiple times. For each example, if the prediction is incorrect, the weights are adjusted based on the learning rate (learning_rate) and the target class label (y). The weights are updated using the formula: self.w[i] += learning_rate * y * x[i].\n",
    "\n",
    "# After training, the weights of the perceptron are printed (print(perceptron.w)), and the perceptron is tested on some test examples using the sign method.\n",
    "\n",
    "# Note: The code assumes that the perceptron is initialized with the number of input features (N). In this case, N is set to 2 because we have two-dimensional input vectors. Adjust the code accordingly if you have a different number of input features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c5136",
   "metadata": {},
   "source": [
    "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª ‚Äì –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ Python\n",
    "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∏–ª–∏ \n",
    "–≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –∏–º–∏—Ç–∏—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞, \n",
    "–ø–æ—Å–∫–æ–ª—å–∫—É –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.\n",
    "–ù–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ç–∏–ø –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π \n",
    "–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–º –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–æ–º (MLP), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è \n",
    "–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. MLP –∏–º–µ–µ—Ç –æ–¥–∏–Ω –≤—Ö–æ–¥–Ω–æ–π\n",
    "—Å–ª–æ–π –∏ –æ–¥–∏–Ω –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π. –ú–µ–∂–¥—É –Ω–∏–º–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ \n",
    "—Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤. –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –∏–º–µ–µ—Ç —Ç–æ—Ç –∂–µ –Ω–∞–±–æ—Ä –Ω–µ–π—Ä–æ–Ω–æ–≤, —á—Ç–æ –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏. \n",
    "–°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞. –ö–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω \n",
    "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ª–∏–Ω–µ–π–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è \n",
    "–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –í—ã—Ö–æ–¥ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –ø–æ–¥–∞–µ—Ç—Å—è –≤ \n",
    "–∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª–æ–µ–≤.\n",
    "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å–ø–æ—Å–æ–±–Ω—ã —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á. –í –æ—Å–Ω–æ–≤–Ω–æ–º –æ–Ω–∏ \n",
    "—Å–æ—Å—Ç–æ—è—Ç –∏–∑ —Ç–∞–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:\n",
    "‚àí –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ø–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö);\n",
    "‚àí —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π (–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ);\n",
    "‚àí –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π. –ß—Ç–æ–±—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ \n",
    "–ø–æ–Ω–∏–º–∞—Ç—å, –∫–∞–∫ –≤–µ–¥—É—Ç —Å–µ–±—è –Ω–µ–π—Ä–æ–Ω—ã. –ù–µ–π—Ä–æ–Ω –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ \n",
    "–ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ö–æ–¥–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–¥–∞–µ—Ç \n",
    "–æ–¥–∏–Ω –≤—ã—Ö–æ–¥. –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–ª–æ–∫–∏ –≤–≤–æ–¥–∞ –∏ \n",
    "–≤—ã–≤–æ–¥–∞, –≥–¥–µ –∫–∞–∂–¥–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–º–µ–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞ (—ç—Ç–æ \n",
    "—Å–∏–ª–∞ —Å–≤—è–∑–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤; —á–µ–º –≤–µ—Å –±–æ–ª—å—à–µ, —Ç–µ–º –æ–¥–∏–Ω –Ω–µ–π—Ä–æ–Ω —Å–∏–ª—å–Ω–µ–µ \n",
    "–≤–ª–∏—è–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π). –î–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤ —É–º–Ω–æ–∂–∞—é—Ç—Å—è –Ω–∞ –≤–µ—Å–∞:\n",
    "‚àí ùë• ‚Üí ùë• ‚àó ùë§1;\n",
    "‚àí ùë¶ ‚Üí ùë¶ ‚àó ùë§2.\n",
    "–í—Ö–æ–¥—ã –ø–æ—Å–ª–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —Å—É–º–º–∏—Ä—É—é—Ç—Å—è —Å –ø—Ä–∏–±–∞–≤–ª–µ–Ω–∏–µ–º –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "–ø–æ—Ä–æ–≥–∞ ¬´c¬ª:\n",
    "ùë•ùë§1 + ùë¶ùë§2 + ùëê\n",
    "–ü–æ–ª—É—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ \n",
    "(—Å–∏–≥–º–æ–∏–¥—É), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥—ã –≤ –æ–¥–∏–Ω –≤—ã—Ö–æ–¥:\n",
    "ùëß = ùëì(ùë•ùë§1 + ùë¶ùë§2 + ùëê).\n",
    "–¢–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç —Å–∏–≥–º–æ–∏–¥–∞:\n",
    "–ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∏–≥–º–æ–∏–¥—ã ‚Äî –æ—Ç 0 –¥–æ 1. –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞ \n",
    "—Å—Ç—Ä–µ–º—è—Ç—Å—è –∫ –Ω—É–ª—é, –∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ ‚Äî –∫ –µ–¥–∏–Ω–∏—Ü–µ.\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä. –ü—É—Å—Ç—å –Ω–µ–π—Ä–æ–Ω –∏–º–µ–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: ùë§ = [0,1] ùëê = 4.\n",
    "–í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: ùë• = 2, ùë¶ = 3.\n",
    "((ùë•ùë§1) + (ùë¶ùë§2)) + ùëê = 20 + 31 + 4 = 7.\n",
    "ùëß = ùëì(7) = 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17beda",
   "metadata": {},
   "source": [
    "1.1.2 –ü—Ä–∏–º–µ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82f851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Value of the neuron: 0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Accepts a number as input and transforms it into a value between 0 and 1\n",
    "# In simpler terms, it normalizes the data\n",
    "def sigmoid(x):\n",
    "    # Activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights  # Parameter used for computing the output\n",
    "        self.bias = bias  # Bias parameter\n",
    "\n",
    "    # Neuron's feedforward step\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) \n",
    "bias = 4 #this is the paramter for controling the sensitivity of activation it is is high If the bias is set to a higher value, it shifts the activation threshold to a higher level, making the neuron more likely to fire even for relatively small inputs. Conversely, if the bias is set to a lower value, the neuron becomes less sensitive and requires stronger inputs to reach the activation threshold.\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2, 3]) \n",
    "output = n.feedforward(x)\n",
    "print(\"Output Value of the neuron:\", output)  # Print the output value of the neuron\n",
    "\n",
    "\n",
    "\n",
    "# In this code, we have a class Neuron that represents a single neuron. The neuron has weights and a bias, which are initialized in the constructor (__init__ method).\n",
    "\n",
    "# The sigmoid function is the activation function used by the neuron. It takes a number as input, applies the sigmoid function, and returns a value between 0 and 1.\n",
    "\n",
    "# The feedforward method of the Neuron class performs the computation step of the neuron. It takes an input vector (inputs), performs the dot product between the weights and the inputs, adds the bias, and then applies the sigmoid function to the result. The output value is returned.\n",
    "\n",
    "# In the main part of the code, we create an instance of the Neuron class with given weights and bias. We also define an input vector x. The feedforward method is called with x as the input, and the output value of the neuron is printed.\n",
    "# In the provided code, the output value of approximately 0.9991 indicates a high activation level or a high probability that the input belongs to the positive class. The closer the output is to 1, the more confident the neuron is in classifying the input as positive. Conversely, if the output were closer to 0, it would indicate a higher probability of the input belonging to the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7636408",
   "metadata": {},
   "source": [
    "–ù–µ–π—Ä–æ—Å–µ—Ç—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –º–µ–∂–¥—É —Å–æ–±–æ–π –Ω–µ–π—Ä–æ–Ω–æ–≤. \n",
    "–ü—Ä–∏–º–µ—Ä –Ω–µ—Å–ª–æ–∂–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
    "–≥–¥–µ:\n",
    "ùë•1, ùë•2 ‚Äî –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π;\n",
    "‚Ñé1, ‚Ñé2 ‚Äî —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π —Å –¥–≤—É–º—è –Ω–µ–π—Ä–æ–Ω–∞–º–∏;\n",
    "ùëú1 ‚Äî –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π.\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä. –ü—Ä–µ–¥—Å—Ç–∞–≤–∏–º, —á—Ç–æ –Ω–µ–π—Ä–æ–Ω—ã –∏–∑ –≥—Ä–∞—Ñ–∏–∫–∞ –≤—ã—à–µ –∏–º–µ—é—Ç –≤–µ—Å–∞ \n",
    "[0, 1]. –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (ùëè) —É –æ–±–æ–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ —Ä–∞–≤–Ω–æ 0 –∏ –æ–Ω–∏ –∏–º–µ—é—Ç \n",
    "–∏–¥–µ–Ω—Ç–∏—á–Ω—É—é —Å–∏–≥–º–æ–∏–¥—É.\n",
    "–ü—Ä–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ùë• = [2, 3] –ø–æ–ª—É—á–∏–º:\n",
    "‚Ñé1 = ‚Ñé2 = ùëì(ùë§ùë• + ùëè) = ùëì((02) + (1 ‚àó 3) + 0) = ùëì(3) = 0.95.\n",
    "ùëú1 = ùëì(ùë§ ‚àó [‚Ñé1, ‚Ñé2] + ùëè) = ùëì((0‚Ñé1) + (1‚Ñé2) + 0) = ùëì(0.95) = 0.72.\n",
    "–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –Ω–µ–π—Ä–æ–Ω–∞–º –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ \n",
    "–ø–æ–ª—É—á–∞—Ç—Å—è –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01e44d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [2 3]\n",
      "Output: 0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Neuron class and sigmoid function definition from previous examples\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "# Neural Network class\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "\n",
    "# Create an instance of the neural network\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "output = network.feedforward(x)\n",
    "print(\"Input:\", x)  # Print the input vector\n",
    "print(\"Output:\", output)  # Print the output value of the neural network\n",
    "#it will print the output for the prediction, for example maybe is like, 2 hours studied and 3 hours slept last night, how much is the possibility to pass the exam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd428b4",
   "metadata": {},
   "source": [
    "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª ‚Äì –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
    "–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Äî —ç—Ç–æ –ø–æ–¥–±–æ—Ä –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤—Å–µ–º \n",
    "–≤—Ö–æ–¥–∞–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.\n",
    "–ö–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏:\n",
    "–ö–∞–∂–¥—ã–π —ç—Ç–∞–ø –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:\n",
    "‚àí –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π –≤—ã—Ö–æ–¥);\n",
    "‚àí –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π).\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä:\n",
    "–î–∞–Ω–∞ –¥–≤—É—Å–ª–æ–π–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å:\n",
    "≈∑ = ùúé(ùë§2ùúé(ùë§1ùë• + ùëè1\n",
    ") + ùëè2\n",
    ").\n",
    "–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–∞ –≤—ã—Ö–æ–¥ ≈∑ –≤–ª–∏—è—é—Ç —Ç–æ–ª—å–∫–æ –¥–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ‚Äî ùë§ (–≤–µ—Å–∞) –∏ ùëè\n",
    "(—Å–º–µ—â–µ–Ω–∏–µ). –ù–∞—Å—Ç—Ä–æ–π–∫—É –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–∞ –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å \n",
    "–æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –º–æ–∂–Ω–æ –∏–∑–æ–±—Ä–∞–∑–∏—Ç—å —Ç–∞–∫:\n",
    "–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ.\n",
    "–ö–∞–∫ –≤–∏–¥–Ω–æ, —Ñ–æ—Ä–º—É–ª–∞ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π \n",
    "–Ω–µ—Å–ª–æ–∂–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ:\n",
    "≈∑ = ùúé(ùë§2ùúé(ùë§1ùë• + ùëè1) + ùëè2)\n",
    "–î–∞–ª–µ–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è. \n",
    "–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —Å–º–µ—â–µ–Ω–∏—è –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –±—É–¥—É—Ç —Ä–∞–≤–Ω—ã 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81370903",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –æ—à–∏–±–∫—É –ø—Ä–æ–≥–Ω–æ–∑–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é \n",
    "–ø–æ—Ç–µ—Ä–∏. –í –ø—Ä–∏–º–µ—Ä–µ —É–º–µ—Å—Ç–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ñ–æ—Ä–º—É–ª–æ–π —Å—É–º–º—ã –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ \n",
    "–æ—à–∏–±–æ–∫ ‚Äî —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–µ–∂–¥—É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–º –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º \n",
    "—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:\n",
    "ùê∏ùëüùëüùëúùëü = ‚àë(ùë¶ ‚àí ùë¶ÃÇ)\n",
    "2\n",
    "ùëõ\n",
    "ùëñ=1\n",
    ".\n",
    "–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ \n",
    "–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–º–µ—Ä–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤ \n",
    "–æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ ‚Äî –æ—Ç –∫–æ–Ω—Ü–∞ –∫ –Ω–∞—á–∞–ª—É, –∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è. \n",
    "–î–ª—è —ç—Ç–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∑–Ω–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä–∏ ‚Äî —Ç–∞–Ω–≥–µ–Ω—Å —É–≥–ª–∞ \n",
    "–Ω–∞–∫–ª–æ–Ω–∞.\n",
    "–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –≤–µ—Å–∞–º –∏ —Å–º–µ—â–µ–Ω–∏—è–º –ø–æ–∑–≤–æ–ª—è–µ—Ç \n",
    "—É–∑–Ω–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Å–æ–≤ \n",
    "–∏ —Å–º–µ—â–µ–Ω–∏–π, –¥–ª—è –µ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–∏:\n",
    "ùêøùëúùë†ùë† (ùë¶, ùë¶ÃÇ) = ‚àë(ùë¶ ‚àí ùë¶ÃÇ)\n",
    "2\n",
    "ùëõ\n",
    "ùëñ=1\n",
    "ùúïùêøùëúùë†ùë† (ùë¶, ùë¶ÃÇ)\n",
    "ùúïùëä\n",
    "=\n",
    "ùúïùêøùëúùë†ùë† (ùë¶, ùë¶ÃÇ)\n",
    "ùúïùë¶ÃÇ\n",
    "‚àô\n",
    "ùúïùë¶ÃÇ\n",
    "ùúïùëß ‚àô\n",
    "ùúïùëß\n",
    "ùúïùëä =\n",
    "= 2(ùë¶ ‚àí ùë¶ÃÇ) ‚àô –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Å–∏–≥–º–æ–∏–¥—ã ‚àô ùë• =\n",
    "= 2(ùë¶ ‚àí ùë¶ÃÇ) ‚àô ùëß(1 ‚àí ùëß) ‚àô ùë•,\n",
    "–≥–¥–µ ùëß = ùëäùë• + ùëè.\n",
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É –ø—Ä–∞–≤–∏–ª—É –º–æ–∂–Ω–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞. –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–¥ \n",
    "Python —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è:\n",
    "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –±–∞–∑–∏—Ä—É—é—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –∏ \n",
    "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö. –°–Ω–∞—á–∞–ª–∞ –º–æ–∂–µ—Ç –∫–∞–∑–∞—Ç—å—Å—è, —á—Ç–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ –Ω–∏—Ö \n",
    "–¥–æ–≤–æ–ª—å–Ω–æ —Å–ª–æ–∂–Ω–æ. –ù–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≥–æ—Ç–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è \n",
    "–¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –Ω–µ —É–≥–ª—É–±–ª—è—Ç—å—Å—è –≤ –∏—Ö \n",
    "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9a161",
   "metadata": {},
   "source": [
    "exercise: \n",
    "\n",
    "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º OurNeuralNetwork. \n",
    "–î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
    "‚àí —Ç—Ä–∏ –≤—Ö–æ–¥–∞ (ùë•1, ùë•2, ùë•3);\n",
    "‚àí —Ç—Ä–∏ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (‚Ñé1, ‚Ñé2, ‚Ñé3);\n",
    "‚àí –≤—ã—Ö–æ–¥ (ùëú1).\n",
    "–ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
    "‚àí ùë§ = [0.5, 0.5, 0.5]\n",
    "‚àí ùëè = 0\n",
    "–î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
    "‚àí –¥–≤–∞ –≤—Ö–æ–¥–∞ (ùë•1, ùë•2);\n",
    "‚àí –¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (‚Ñé1, ‚Ñé2);\n",
    "‚àí –¥–≤–∞ –≤—ã—Ö–æ–¥–∞ (ùëú1, ùëú2).\n",
    "–ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
    "‚àí ùë§ = [1, 0];\n",
    "‚àí ùëè = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb9c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.1 0.2 0.3]\n",
      "Output: 0.7030081474216077\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with Three Inputs, Three Hidden Neurons, and One Output\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid activation function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        # Calculate the total input to the neuron\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        # Apply the sigmoid activation function to the total\n",
    "        return sigmoid(total)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5, 0.5, 0.5])  # Weights for the three input features\n",
    "        bias = 0\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.h3 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # Feedforward through the neural network\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "# Create an instance of the neural network\n",
    "network = NeuralNetwork()\n",
    "x = np.array([0.1, 0.2, 0.3])  # Three input values\n",
    "output = network.feedforward(x)  # Feedforward the inputs through the network\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c634f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: 0.1\n",
      "Input 2: 0.2\n",
      "Output 1: 0.9890491628120831\n",
      "Output 2: 0.9890491628120831\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with Two Inputs, Two Hidden Neurons, and Two Outputs:\n",
    "import numpy as np\n",
    "\n",
    "# Hyperbolic tangent activation function\n",
    "\n",
    "# The hyperbolic tangent (tanh) activation function is commonly used in neural networks as an alternative to\n",
    "# the sigmoid activation function. It has a range between -1 and 1,\n",
    "# which allows the activation of neurons to be centered around zero.\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Neuron class\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return tanh(total)\n",
    "\n",
    "# NeuralNetwork class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights_h = np.array([1, 0])  # Weights for the two input features in hidden layer\n",
    "        weights_o = np.array([1, 1])  # Weights for the two inputs from hidden layer to output layer\n",
    "        bias = 1\n",
    "        self.h1 = Neuron(weights_h, bias)\n",
    "        self.h2 = Neuron(weights_h, bias)\n",
    "        self.o1 = Neuron(weights_o, bias)\n",
    "        self.o2 = Neuron(weights_o, bias)\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        out_h1 = self.h1.feedforward(np.array([x1, x2]))\n",
    "        out_h2 = self.h2.feedforward(np.array([x1, x2]))\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "\n",
    "# Create an instance of the neural network\n",
    "network = NeuralNetwork()\n",
    "x1 = 0.1\n",
    "x2 = 0.2\n",
    "output1, output2 = network.feedforward(x1, x2)  # Feedforward the inputs through the network\n",
    "print(\"Input 1:\", x1)\n",
    "print(\"Input 2:\", x2)\n",
    "print(\"Output 1:\", output1)\n",
    "print(\"Output 2:\", output2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fddee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: 0.1\n",
      "Input 2: -0.2\n",
      "Output 1: 3.2\n",
      "Output 2: 3.2\n"
     ]
    }
   ],
   "source": [
    "#  The ReLU function is computationally efficient and easy to implement. It simply returns the input value if it is positive, and zero otherwise.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Rectified Linear Unit (ReLU) activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Neuron class\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return relu(total)\n",
    "\n",
    "# NeuralNetwork class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights_h = np.array([[1, 0], [1, 0]])  # Weights for the two input features in hidden layer\n",
    "        weights_o = np.array([[1, 1], [1, 1]])  # Weights for the two inputs from hidden layer to output layer\n",
    "        bias = 1\n",
    "        self.h1 = Neuron(weights_h[0], bias)\n",
    "        self.h2 = Neuron(weights_h[1], bias)\n",
    "        self.o1 = Neuron(weights_o[0], bias)\n",
    "        self.o2 = Neuron(weights_o[1], bias)\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        out_h1 = self.h1.feedforward(np.array([x1, x2]))\n",
    "        out_h2 = self.h2.feedforward(np.array([x1, x2]))\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "\n",
    "# Create an instance of the neural network\n",
    "network = NeuralNetwork()\n",
    "x1 = 0.1\n",
    "x2 = -0.2\n",
    "output1, output2 = network.feedforward(x1, x2)  # Feedforward the inputs through the network\n",
    "print(\"Input 1:\", x1)\n",
    "print(\"Input 2:\", x2)\n",
    "print(\"Output 1:\", output1)\n",
    "print(\"Output 2:\", output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f59c8",
   "metadata": {},
   "source": [
    "1.2. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å –ø–æ–º–æ—â—å—é Scikit-Learn –≤ Python\n",
    "–¢–µ–ø–µ—Ä—å –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –∫–∞–∫–∏–µ —à–∞–≥–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ \n",
    "–≤—ã–ø–æ–ª–Ω–∏—Ç—å, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å —Å –ø–ª–æ—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏. \n",
    "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è \n",
    "–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –¥–∞–Ω–Ω–æ–µ —Ä–∞—Å—Ç–µ–Ω–∏–µ –∏—Ä–∏—Å–∞. –ú—ã \n",
    "–±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É Python Scikit-Learn –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—à–µ–π \n",
    "–Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "Sklearn –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç 2 –æ—Ü–µ–Ω—â–∏–∫–∞ –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ \n",
    "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ:\n",
    "‚àí MLPClassifier;\n",
    "‚àí MLPRegressor \n",
    "–ù–∞—á–Ω–µ–º —Å –∏–º–ø–æ—Ä—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫.\n",
    "MLPClassifier\n",
    "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "–ú—ã –±—É–¥–µ–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ñ—Ä: –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ñ—Ä, \n",
    "–∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º 8x8 –¥–ª—è —Ü–∏—Ñ—Ä 0-9. –ù–∏–∂–µ –º—ã –±—É–¥–µ–º \n",
    "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ü–∏—Ñ—Ä–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–ª—å–µ –≤ –ë–æ—Å—Ç–æ–Ω–µ: –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä \n",
    "–¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–ª—å–µ –≤ –ë–æ—Å—Ç–æ–Ω–µ, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö \n",
    "—Å–≤–æ–π—Å—Ç–≤–∞—Ö –¥–æ–º–∞, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç, —É—Ä–æ–≤–µ–Ω—å –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ \n",
    "–Ω–∞ –¥—É—à—É –Ω–∞—Å–µ–ª–µ–Ω–∏—è –≤ –≥–æ—Ä–æ–¥–µ –∏ —Ç. –¥. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –∑–∞–¥–∞—á \n",
    "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n",
    "Sklearn –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∞ —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å \n",
    "–∏—Ö, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥—ã load_digits() –∏ load_boston().\n",
    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "MLPClassifier ‚Äî —ç—Ç–æ –∫–ª–≤—Å—Å, –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–∞–∫ —á–∞—Å—Ç—å –º–æ–¥—É–ª—è neuro_network \n",
    "sklearn –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º \n",
    "–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞.\n",
    "–ö–∞–∫ –æ–±—ã—á–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏:\n",
    "‚àí –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –º–æ–¥–µ–ª–∏ \n",
    "–æ–±—É—á–µ–Ω–∏—è;\n",
    "‚àí —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å \n",
    "–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
    "–§—É–Ω–∫—Ü–∏—è train_test_split –º–æ–¥—É–ª—è model_selection sklearn –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º \n",
    "—Ä–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: 80% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ 20% –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. \n",
    "–ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º seed(random_state=123) —Å train_test_split, —á—Ç–æ–±—ã –º—ã \n",
    "–≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞–ª–∏ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –º–æ–≥–ª–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∏ \n",
    "–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±—É–¥—É—â–µ–º.\n",
    "–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å MLPClassifier —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é \n",
    "–¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "C–æ–∑–¥–∞–¥–∏–º –º–µ—Ç–æ–¥ plot_confusion_matrix(), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –∏ \n",
    "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º –æ–Ω —Å—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É –ø—É—Ç–∞–Ω–∏—Ü—ã, \n",
    "–∏—Å–ø–æ–ª—å–∑—É—è matplotlib\n",
    "MLPRegressor ‚Äî —ç—Ç–æ –∫–ª–∞—Å—Å, –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–∞–∫ —á–∞—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ \n",
    "neuro_network sklearn –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º \n",
    "–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞. –¢–∞–∫–∂–µ —Ä–∞–∑–¥–µ–ª–∏–º –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏:\n",
    "‚àí –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è (80%), –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è \n",
    "–º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è;\n",
    "‚àí —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (20%), –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å \n",
    "–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d626a55",
   "metadata": {},
   "source": [
    "example: \n",
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã MLPClassified –∏ MLPRegressor –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ \n",
    "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ \n",
    "–∞—Ç—Ä–∏–±—É—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–µ—Ç–µ –≤–∑—è—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ò—Ä–∏—Å–æ–≤:\n",
    "https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f\n",
    "7d537619d0e07d5ae3/iris.csv\n",
    "–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –æ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã:\n",
    "https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear\u0002regression/master/Salary_Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8284dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
       "0             5.1          3.5           1.4          0.2     Setosa\n",
       "1             4.9          3.0           1.4          0.2     Setosa\n",
       "2             4.7          3.2           1.3          0.2     Setosa\n",
       "3             4.6          3.1           1.5          0.2     Setosa\n",
       "4             5.0          3.6           1.4          0.2     Setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  Virginica\n",
       "146           6.3          2.5           5.0          1.9  Virginica\n",
       "147           6.5          3.0           5.2          2.0  Virginica\n",
       "148           6.2          3.4           5.4          2.3  Virginica\n",
       "149           5.9          3.0           5.1          1.8  Virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42670ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:  (150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns={'variety': 'target'})\n",
    "X_df, Y_df = df.drop(['target'], axis=1), df.target\n",
    "print('Dataset Size: ', X_df.shape, Y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda1b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Sizes :  (120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size=0.80, test_size=0.20, stratify=Y_df,\n",
    "                                                    random_state=123)\n",
    "print('Train/Test Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10491391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, random_state=123)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(max_iter=1000, random_state=123)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create an instance of MLPClassifier with specified parameters\n",
    "mlp_classifier = MLPClassifier(random_state=123, max_iter=1000) #the seed to gain the same result everytime it runs \n",
    "\n",
    "# Train the classifier using the training data\n",
    "mlp_classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22a02d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  8  2]\n",
      " [ 0  0 10]]\n",
      "Loss: 0.06237838693566958\n",
      "Number of Coefs: 2\n",
      "Number of Intercepts: 2\n",
      "Number of Iterations for which Estimator Ran: 641\n",
      "Name of Output Layer Activation Function: softmax\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 345.6x345.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(Y_test, Y_preds):\n",
    "    # Compute confusion matrix\n",
    "    conf_mat = confusion_matrix(Y_test, Y_preds)\n",
    "    print(conf_mat)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.matshow(conf_mat, cmap=plt.cm.Blues, fignum=1)\n",
    "    plt.yticks(range(3), range(3))\n",
    "    plt.xticks(range(3), range(3))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add text annotations to the plot\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            plt.text(i - 0.2, j + 0.1, str(conf_mat[j, i]), color='tab:red')\n",
    "\n",
    "# Call the function to plot confusion matrix\n",
    "plot_confusion_matrix(Y_test, mlp_classifier.predict(X_test))\n",
    "\n",
    "# Print additional information\n",
    "print('Loss:', mlp_classifier.loss_)\n",
    "print('Number of Coefs:', len(mlp_classifier.coefs_))\n",
    "print('Number of Intercepts:', len(mlp_classifier.intercepts_))\n",
    "print('Number of Iterations for which Estimator Ran:', mlp_classifier.n_iter_)\n",
    "print('Name of Output Layer Activation Function:', mlp_classifier.out_activation_)\n",
    "\n",
    "\n",
    "# The confusion matrix is a tabular representation that summarizes the performance of a classification model on a dataset. \n",
    "# It provides a detailed breakdown of the model's predictions and the actual labels of the data.\n",
    "# The confusion matrix has a square shape, where the rows represent the actual or true labels \n",
    "# of the data, and the columns represent the predicted labels by the model. Each cell in the matrix represents the\n",
    "# count or frequency of samples falling into a particular combination of true and predicted labels.\n",
    "# The confusion matrix allows you to derive several important evaluation metrics for assessing the model's performance, including:\n",
    "\n",
    "# True Positives (TP): The number of samples that are correctly predicted as positive for a specific class.\n",
    "# True Negatives (TN): The number of samples that are correctly predicted as negative for a specific class.\n",
    "# False Positives (FP): The number of samples that are incorrectly predicted as positive (Type I error) for a specific class.\n",
    "# False Negatives (FN): The number of samples that are incorrectly predicted as negative (Type II error) for a specific class.\n",
    "# From the confusion matrix, you can calculate various performance metrics, such as:\n",
    "\n",
    "# Accuracy: Overall correctness of the model's predictions (sum of diagonal cells divided by the total number of samples).\n",
    "# Precision: The ability of the model to correctly identify positive predictions (TP / (TP + FP)).\n",
    "# Recall (Sensitivity or True Positive Rate): The proportion of actual positive samples that are correctly identified by the model (TP / (TP + FN)).\n",
    "# Specificity (True Negative Rate): The proportion of actual negative samples that are correctly identified by the model (TN / (TN + FP)).\n",
    "# F1-score: The harmonic mean of precision and recall, providing a balanced measure between the two metrics.\n",
    "# Analyzing the confusion matrix helps you understand the model's performance in terms of correctly and incorrectly classified samples for each class. It allows you to identify common sources of misclassification, such as false positives and false negatives, which can guide you in improving the model or making decisions based on its predictions.\n",
    "\n",
    "\n",
    "# Loss: The loss value (0.06884106415956573) represents the objective or cost function of the MLPClassifier. It indicates the measure of inconsistency between the predicted class probabilities and the true labels. Lower values indicate better model performance.\n",
    "# Number of Coefs: The number of coefs (2) refers to the number of coefficients or weights in the model. In this case, it indicates the total number of weights associated with the connections between the input and hidden layers, and between the hidden and output layers.\n",
    "# Number of Intercepts: The number of intercepts (2) represents the number of bias terms in the model. Bias terms are additional parameters added to each neuron, allowing the model to learn offsets or shifts in the decision boundaries\n",
    "# Number of Iterations for which Estimator Ran: The number of iterations (630) signifies the total number of training iterations or epochs the MLPClassifier performed during the training process. Each iteration involves presenting the training data to the model and adjusting the weights to minimize the loss.\n",
    "# Name of Output Layer Activation Function: The output layer activation function (softmax) indicates the type of activation function used in the output layer of the MLPClassifier. Softmax is commonly used for multi-class classification problems as it produces probabilities for each class, ensuring that the predicted class probabilities sum up to 1.\n",
    "# These values provide insights into the properties and behavior of the trained MLPClassifier model, such as its performance (loss), complexity (number of coefs and intercepts), convergence (number of iterations), and the activation function used in the output layer.\n",
    "# These values indicate the number of instances that were correctly classified for each respective class. In other words, they represent the count of true positives for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0cf2c",
   "metadata": {},
   "source": [
    "–†–µ–≥—Ä–µ—Å—Å–æ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674201cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearsExperience   Salary\n",
       "0              1.1  39343.0\n",
       "1              1.3  46205.0\n",
       "2              1.5  37731.0\n",
       "3              2.0  43525.0\n",
       "4              2.2  39891.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e90b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test size:  (120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size = 0.80, test_size = 0.20, random_state = 123)\n",
    "print ('Train/Test size: ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f353c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# # Create an instance of MLPRegressor\n",
    "# mlp_regressor = MLPRegressor(random_state=123, max_iter=35000)\n",
    "\n",
    "# # # Fit the model to the training data\n",
    "# mlp_regressor.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# # Classifier: A classifier is used for solving classification problems, where the task is to predict the class or category of a given input. The output of a classifier is a discrete label or class. For example, classifying an email as \"spam\" or \"not spam\" or categorizing images into different classes such as \"cat,\" \"dog,\" or \"bird.\"\n",
    "\n",
    "# # Regressor: A regressor is used for solving regression problems, where the task is to predict a continuous numerical value or quantity. The output of a regressor is a continuous value. For example, predicting the price of a house based on its features, estimating the temperature based on historical data, or forecasting stock prices.\n",
    "\n",
    "# # In terms of algorithms and techniques, classifiers and regressors often utilize similar machine learning models, such as neural networks, decision trees, support vector machines, etc. However, their training and evaluation processes may differ, as well as the evaluation metrics used to assess their performance.\n",
    "\n",
    "# # In scikit-learn, the MLPClassifier class is specifically designed for classification tasks, while the MLPRegressor class is designed for regression tasks. They have different default activation functions for the output layer (softmax for MLPClassifier and identity for MLPRegressor), and their loss functions and evaluation metrics are tailored accordingly. However, you can still customize the architecture and other parameters to fit your specific problem within the respective framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e09dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_preds = mlp_regressor.predict(X_test)\n",
    "\n",
    "# print (Y_preds[:10])\n",
    "# print (Y_test[:10])\n",
    "# print ('Test R^2 Score: %.3f'%mlp_regressor.score(X_test, Y_test))\n",
    "# print ('Training R^2 Score: %.3f'%mlp_regressor.score(X_train, Y_train))\n",
    "\n",
    "# print ('Loss: ', mlp_regressor.loss_)\n",
    "# print ('Number of Coefs: ', len(mlp_regressor.coefs_))\n",
    "# print ('Number of Intercepts: ', len(mlp_regressor.intercepts_))\n",
    "# print ('Number of Iteration for Which Estimator Ran: ', mlp_regressor.n_iter_)\n",
    "# print ('Name of Output Layer Activation Function: ', mlp_regressor.out_activation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3ec5a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004be93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
